{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765434d0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:41.548738Z",
     "iopub.status.busy": "2025-10-19T17:57:41.548409Z",
     "iopub.status.idle": "2025-10-19T17:57:57.924162Z",
     "shell.execute_reply": "2025-10-19T17:57:57.923263Z"
    },
    "papermill": {
     "duration": 16.381474,
     "end_time": "2025-10-19T17:57:57.925721",
     "exception": false,
     "start_time": "2025-10-19T17:57:41.544247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dacd3b",
   "metadata": {
    "papermill": {
     "duration": 0.002329,
     "end_time": "2025-10-19T17:57:57.931039",
     "exception": false,
     "start_time": "2025-10-19T17:57:57.928710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5363bc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:57.937047Z",
     "iopub.status.busy": "2025-10-19T17:57:57.936560Z",
     "iopub.status.idle": "2025-10-19T17:57:58.001320Z",
     "shell.execute_reply": "2025-10-19T17:57:58.000465Z"
    },
    "papermill": {
     "duration": 0.06908,
     "end_time": "2025-10-19T17:57:58.002479",
     "exception": false,
     "start_time": "2025-10-19T17:57:57.933399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "Selected model type: KORA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 0. Configuration\n",
    "# ==============================================================================\n",
    "# You can change these settings in your Kaggle notebook\n",
    "CONFIG = {\n",
    "    \"model_type\": \"kora\",  # \"lora\" or \"kora\"\n",
    "    \"vit_model\": \"vit_base_patch16_224.augreg_in21k_ft_in1k\",\n",
    "    \"img_size\": 224,\n",
    "    \"lora_rank\": 8,\n",
    "    \"num_classes_cifar100\": 100,\n",
    "    \"num_classes_tiny_imagenet\": 200,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"alpha\": 1.0, # Scaling factor for adapter outputs\n",
    "    \"composition_dim\": 4, # d_comp for KoRA\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"tiny_imagenet_dir\": \"/kaggle/input/tiny-imagenet/tiny-imagenet-200\",\n",
    "}\n",
    "\n",
    "print(f\"Running on device: {CONFIG['device']}\")\n",
    "print(f\"Selected model type: {CONFIG['model_type'].upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2160e32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:58.008855Z",
     "iopub.status.busy": "2025-10-19T17:57:58.008379Z",
     "iopub.status.idle": "2025-10-19T17:57:58.015578Z",
     "shell.execute_reply": "2025-10-19T17:57:58.015069Z"
    },
    "papermill": {
     "duration": 0.011322,
     "end_time": "2025-10-19T17:57:58.016562",
     "exception": false,
     "start_time": "2025-10-19T17:57:58.005240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 1. CKA (Centered Kernel Alignment) Implementation for H1a\n",
    "# ==============================================================================\n",
    "# This helps us measure if KoRA creates more compositional representations\n",
    "def gram_linear(x):\n",
    "    \"\"\"Compute Gram matrix.\"\"\"\n",
    "    return torch.mm(x, x.T)\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "    \"\"\"Center a Gram matrix.\"\"\"\n",
    "    if not torch.allclose(gram, gram.T, atol=1e-5):\n",
    "        raise ValueError(\"Input must be a symmetric matrix.\")\n",
    "    \n",
    "    n = gram.shape[0]\n",
    "    if unbiased:\n",
    "        I = torch.eye(n, device=gram.device)\n",
    "        one = torch.ones(n, 1, device=gram.device)\n",
    "        gram -= (torch.mm(one, one.T) * gram) / (n - 1)\n",
    "        gram -= (torch.mm(gram, one @ one.T)) / (n - 1)\n",
    "        gram += (torch.sum(gram) * one @ one.T) / ((n - 1) * (n - 2))\n",
    "    else:\n",
    "        mean_row = torch.mean(gram, dim=1, keepdim=True)\n",
    "        mean_all = torch.mean(gram)\n",
    "        gram -= mean_row\n",
    "        gram -= mean_row.T\n",
    "        gram += mean_all\n",
    "    return gram\n",
    "\n",
    "def cka(x, y, unbiased=False):\n",
    "    \"\"\"Compute CKA score between two matrices of activations.\"\"\"\n",
    "    x = x.view(x.shape[0], -1).to(CONFIG['device'])\n",
    "    y = y.view(y.shape[0], -1).to(CONFIG['device'])\n",
    "    \n",
    "    gram_x = center_gram(gram_linear(x), unbiased=unbiased)\n",
    "    gram_y = center_gram(gram_linear(y), unbiased=unbiased)\n",
    "\n",
    "    # HSIC (Hilbert-Schmidt Independence Criterion)\n",
    "    hsic = torch.sum(gram_x * gram_y)\n",
    "    var1 = torch.sqrt(torch.sum(gram_x * gram_x))\n",
    "    var2 = torch.sqrt(torch.sum(gram_y * gram_y))\n",
    "\n",
    "    return (hsic / (var1 * var2)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ffb13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:58.022339Z",
     "iopub.status.busy": "2025-10-19T17:57:58.022133Z",
     "iopub.status.idle": "2025-10-19T17:57:58.032025Z",
     "shell.execute_reply": "2025-10-19T17:57:58.031454Z"
    },
    "papermill": {
     "duration": 0.014023,
     "end_time": "2025-10-19T17:57:58.033004",
     "exception": false,
     "start_time": "2025-10-19T17:57:58.018981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 2. Data Handling (CIFAR-100 and Tiny ImageNet)\n",
    "# ==============================================================================\n",
    "def get_cifar100_loaders(data_dir, img_size, batch_size):\n",
    "    \"\"\"Prepare CIFAR-100 dataloaders.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n",
    "    ])\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    train_dataset = CIFAR100(root=data_dir, train=True, download=True, transform=transform)\n",
    "    test_dataset = CIFAR100(root=data_dir, train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def download_and_extract_tiny_imagenet(data_dir, tiny_dir):\n",
    "    \"\"\"Download and prepare Tiny ImageNet dataset if not present.\"\"\"\n",
    "    if os.path.exists(tiny_dir):\n",
    "        print(\"Tiny ImageNet already downloaded.\")\n",
    "        return\n",
    "        \n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(data_dir, \"tiny-imagenet-200.zip\")\n",
    "    \n",
    "    print(\"Downloading Tiny ImageNet...\")\n",
    "    # Kaggle notebooks might need specific proxy settings if this fails.\n",
    "    # Using requests with stream=True for large files.\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    \n",
    "    print(\"Extracting Tiny ImageNet...\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "\n",
    "    # The validation directory needs restructuring for ImageFolder\n",
    "    val_dir = os.path.join(tiny_dir, 'val')\n",
    "    val_annotations = os.path.join(val_dir, 'val_annotations.txt')\n",
    "    with open(val_annotations, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.split('\\t')\n",
    "            img_file, class_id = parts[0], parts[1]\n",
    "            class_dir = os.path.join(val_dir, class_id)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            os.rename(os.path.join(val_dir, 'images', img_file), os.path.join(class_dir, img_file))\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"Tiny ImageNet setup complete.\")\n",
    "\n",
    "def get_tiny_imagenet_loaders(data_dir, tiny_dir, img_size, batch_size):\n",
    "    \"\"\"Prepare Tiny ImageNet dataloaders.\"\"\"\n",
    "    download_and_extract_tiny_imagenet(data_dir, tiny_dir)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # The validation directory is already structured correctly for ImageDataset.\n",
    "    # We pass the transform directly to it here.\n",
    "    val_dataset_path = os.path.join(tiny_dir, 'val')\n",
    "    full_val_dataset = timm.data.ImageDataset(val_dataset_path, transform=transform)\n",
    "    \n",
    "    # We can create a Subset from this dataset.\n",
    "    test_dataset = torch.utils.data.Subset(\n",
    "        full_val_dataset,\n",
    "        np.arange(len(full_val_dataset)) # Use the actual length of the dataset\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return test_loader\n",
    "# def get_tiny_imagenet_loaders(data_dir, tiny_dir, img_size, batch_size):\n",
    "#     \"\"\"Prepare Tiny ImageNet dataloaders.\"\"\"\n",
    "#     download_and_extract_tiny_imagenet(data_dir, tiny_dir)\n",
    "    \n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((img_size, img_size)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "\n",
    "#     # The validation set is now the official test set for transfer learning\n",
    "#     test_dataset = torch.utils.data.Subset(\n",
    "#         timm.data.ImageDataset(os.path.join(tiny_dir, 'val')),\n",
    "#         np.arange(10000) # Full validation set\n",
    "#     )\n",
    "#     for i, (path, target) in enumerate(test_dataset.dataset.parser.samples):\n",
    "#         test_dataset.dataset.parser.samples[i] = (path, target)\n",
    "\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "#     return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8496d59c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:58.038822Z",
     "iopub.status.busy": "2025-10-19T17:57:58.038581Z",
     "iopub.status.idle": "2025-10-19T17:57:58.056218Z",
     "shell.execute_reply": "2025-10-19T17:57:58.055512Z"
    },
    "papermill": {
     "duration": 0.022018,
     "end_time": "2025-10-19T17:57:58.057360",
     "exception": false,
     "start_time": "2025-10-19T17:57:58.035342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Model Architecture (LoRA and KoRA)\n",
    "# ==============================================================================\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"A drop-in replacement for nn.Linear with LoRA.\"\"\"\n",
    "    def __init__(self, in_features, out_features, rank):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lora_B @ (self.lora_A @ x)\n",
    "\n",
    "class KoRAViT(nn.Module):\n",
    "    \"\"\"KoRA-enabled Vision Transformer.\"\"\"\n",
    "    def __init__(self, vit_model_name, rank, num_classes, composition_dim, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.backbone = timm.create_model(vit_model_name, pretrained=True)\n",
    "        \n",
    "        # Freeze backbone parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.lora_adapters = nn.ModuleList()\n",
    "        self.adapter_projections = nn.ModuleList()\n",
    "        self.captured_inputs = []\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Identify target layers (attention and MLP) and attach hooks/adapters\n",
    "        for block in self.backbone.blocks:\n",
    "            # --- Attention layers ---\n",
    "            # QKV\n",
    "            qkv_layer = block.attn.qkv\n",
    "            self.add_adapter_and_hook(qkv_layer, rank, composition_dim)\n",
    "            # Proj\n",
    "            proj_layer = block.attn.proj\n",
    "            self.add_adapter_and_hook(proj_layer, rank, composition_dim)\n",
    "            \n",
    "            # --- MLP layers ---\n",
    "            fc1_layer = block.mlp.fc1\n",
    "            self.add_adapter_and_hook(fc1_layer, rank, composition_dim)\n",
    "            fc2_layer = block.mlp.fc2\n",
    "            self.add_adapter_and_hook(fc2_layer, rank, composition_dim)\n",
    "            \n",
    "        # --- Compositional Layers (Psi and Phi) ---\n",
    "        num_adapters = len(self.lora_adapters)\n",
    "        # Psi: Couplings between composed features\n",
    "        self.psi_couplings = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(composition_dim, composition_dim), nn.GELU())\n",
    "            for _ in range(num_adapters - 1)\n",
    "        ])\n",
    "        # Phi: Composers for the sum of current and propagated info\n",
    "        self.phi_composers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(composition_dim, composition_dim), nn.GELU())\n",
    "            for _ in range(num_adapters)\n",
    "        ])\n",
    "\n",
    "        # New classification head\n",
    "        backbone_dim = self.backbone.head.in_features\n",
    "        self.head = nn.Linear(backbone_dim, num_classes)\n",
    "        \n",
    "        print(f\"Initialized KoRA with {len(self.lora_adapters)} adapters.\")\n",
    "\n",
    "    def add_adapter_and_hook(self, layer, rank, composition_dim):\n",
    "        # Hook to capture input to the linear layer\n",
    "        def hook_fn(module, input, output):\n",
    "            self.captured_inputs.append(input[0].detach())\n",
    "\n",
    "        self.hooks.append(layer.register_forward_hook(hook_fn))\n",
    "        \n",
    "        # Create LoRA adapter\n",
    "        adapter = LoRALayer(layer.in_features, layer.out_features, rank)\n",
    "        self.lora_adapters.append(adapter)\n",
    "        \n",
    "        # Create projection to common composition dimension\n",
    "        projection = nn.Linear(layer.out_features, composition_dim, bias=False)\n",
    "        self.adapter_projections.append(projection)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     self.captured_inputs = []\n",
    "    #     # Run backbone forward pass to trigger hooks and get CLS token features\n",
    "    #     # Note: we use forward_features to get the CLS token before the final head\n",
    "    #     backbone_features = self.backbone.forward_features(x)\n",
    "\n",
    "    #     # Compute adapter outputs from captured inputs\n",
    "    #     adapter_outputs = [\n",
    "    #         adapter(inp.transpose(-1, -2)).transpose(-1, -2)\n",
    "    #         for adapter, inp in zip(self.lora_adapters, self.captured_inputs)\n",
    "    #     ]\n",
    "        \n",
    "    #     # Project adapter outputs to common dimension\n",
    "    #     projected_outputs = [\n",
    "    #         proj(out) for proj, out in zip(self.adapter_projections, adapter_outputs)\n",
    "    #     ]\n",
    "        \n",
    "    #     # Aggregate projected outputs (we'll average over sequence length for simplicity)\n",
    "    #     agg_projected_outputs = [torch.mean(p_out, dim=1) for p_out in projected_outputs]\n",
    "\n",
    "    #     # --- KoRA Composition Logic ---\n",
    "    #     # Initialize with the first composed adapter output\n",
    "    #     composed_features = self.phi_composers[0](agg_projected_outputs[0])\n",
    "        \n",
    "    #     # Sequentially compose the rest\n",
    "    #     for i in range(1, len(agg_projected_outputs)):\n",
    "    #         propagated_features = self.psi_couplings[i - 1](composed_features)\n",
    "    #         combined_features = agg_projected_outputs[i] + propagated_features\n",
    "    #         composed_features = self.phi_composers[i](combined_features)\n",
    "            \n",
    "    #     # Add the final composed delta to the backbone's CLS token features\n",
    "    #     # We need a final projection from composition_dim to the backbone's dimension\n",
    "    #     if not hasattr(self, 'final_projection'):\n",
    "    #         self.final_projection = nn.Linear(composed_features.shape[-1], backbone_features.shape[-1]).to(x.device)\n",
    "\n",
    "    #     composed_delta = self.final_projection(composed_features)\n",
    "        \n",
    "    #     final_features = backbone_features + self.alpha * composed_delta\n",
    "        \n",
    "    #     # Classification\n",
    "    #     logits = self.head(final_features)\n",
    "        \n",
    "    #     return logits\n",
    "#\n",
    "# REPLACE this method in your KoRAViT class\n",
    "#\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.captured_inputs = []\n",
    "        \n",
    "        # Run backbone forward pass to get the full sequence of token features\n",
    "        # Shape: [batch_size, num_patches + 1, hidden_dim]\n",
    "        backbone_output_sequence = self.backbone.forward_features(x)\n",
    "    \n",
    "        # The CLS token is the first token in the sequence\n",
    "        cls_token = backbone_output_sequence[:, 0]\n",
    "    \n",
    "        # Compute adapter outputs from captured inputs (this part is correct)\n",
    "        adapter_outputs = [\n",
    "            adapter(inp.transpose(-1, -2)).transpose(-1, -2)\n",
    "            for adapter, inp in zip(self.lora_adapters, self.captured_inputs)\n",
    "        ]\n",
    "        \n",
    "        # Project adapter outputs to common dimension\n",
    "        projected_outputs = [\n",
    "            proj(out) for proj, out in zip(self.adapter_projections, adapter_outputs)\n",
    "        ]\n",
    "        \n",
    "        # Aggregate projected outputs by averaging over the sequence length\n",
    "        agg_projected_outputs = [torch.mean(p_out, dim=1) for p_out in projected_outputs]\n",
    "    \n",
    "        # --- KoRA Composition Logic (this part is correct) ---\n",
    "        composed_features = self.phi_composers[0](agg_projected_outputs[0])\n",
    "        for i in range(1, len(agg_projected_outputs)):\n",
    "            propagated_features = self.psi_couplings[i - 1](composed_features)\n",
    "            combined_features = agg_projected_outputs[i] + propagated_features\n",
    "            composed_features = self.phi_composers[i](combined_features)\n",
    "            \n",
    "        # --- The Fix is Here ---\n",
    "        # Project the final composed features to the backbone's hidden dimension\n",
    "        if not hasattr(self, 'final_projection'):\n",
    "            # Ensure the projection layer is created on the correct device\n",
    "            self.final_projection = nn.Linear(composed_features.shape[-1], cls_token.shape[-1]).to(x.device)\n",
    "    \n",
    "        composed_delta = self.final_projection(composed_features)\n",
    "        \n",
    "        # Add the delta ONLY to the CLS token\n",
    "        # Both tensors now have shape [batch_size, hidden_dim]\n",
    "        final_cls_token = cls_token + self.alpha * composed_delta\n",
    "        \n",
    "        # Pass the modified CLS token to the classification head\n",
    "        logits = self.head(final_cls_token)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    \n",
    "    def __del__(self):\n",
    "        # Clean up hooks when the model is deleted\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "class LoRAViT(nn.Module):\n",
    "    \"\"\"A standard LoRA-enabled ViT for baseline comparison.\"\"\"\n",
    "    def __init__(self, vit_model_name, rank, num_classes, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.backbone = timm.create_model(vit_model_name, pretrained=True)\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        \n",
    "        # Inject LoRA layers\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                self.lora_layers[name.replace('.', '_')] = LoRALayer(\n",
    "                    module.in_features, module.out_features, rank\n",
    "                )\n",
    "        \n",
    "        self.hooks = []\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                hook = self.create_hook(name.replace('.', '_'))\n",
    "                self.hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "        # New classification head\n",
    "        self.head = nn.Linear(self.backbone.head.in_features, num_classes)\n",
    "        print(f\"Initialized LoRA with {len(self.lora_layers)} adapters.\")\n",
    "\n",
    "    def create_hook(self, name):\n",
    "        def hook_fn(module, input, output):\n",
    "            lora_output = self.lora_layers[name](input[0].transpose(-1, -2)).transpose(-1, -2)\n",
    "            return output + self.alpha * lora_output\n",
    "        return hook_fn\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     return self.head(self.backbone.forward_features(x))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "    # Get the features for all tokens from the backbone\n",
    "        all_token_features = self.backbone.forward_features(x)\n",
    "    \n",
    "    # Select only the CLS token, which is the first token in the sequence [:, 0]\n",
    "        cls_token = all_token_features[:, 0]\n",
    "    \n",
    "    # Pass the single CLS token to the head for classification\n",
    "        return self.head(cls_token)\n",
    "\n",
    "    \n",
    "    def __del__(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84afbf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:58.063586Z",
     "iopub.status.busy": "2025-10-19T17:57:58.063072Z",
     "iopub.status.idle": "2025-10-19T17:57:58.069574Z",
     "shell.execute_reply": "2025-10-19T17:57:58.069027Z"
    },
    "papermill": {
     "duration": 0.010721,
     "end_time": "2025-10-19T17:57:58.070727",
     "exception": false,
     "start_time": "2025-10-19T17:57:58.060006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 4. Training and Evaluation Loops\n",
    "# ==============================================================================\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd783dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:58.076686Z",
     "iopub.status.busy": "2025-10-19T17:57:58.076480Z",
     "iopub.status.idle": "2025-10-19T17:57:58.091676Z",
     "shell.execute_reply": "2025-10-19T17:57:58.090928Z"
    },
    "papermill": {
     "duration": 0.019689,
     "end_time": "2025-10-19T17:57:58.092850",
     "exception": false,
     "start_time": "2025-10-19T17:57:58.073161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 5. Main Experiment Orchestration\n",
    "# ==============================================================================\n",
    "def run_experiment():\n",
    "    # --- 1. Setup Data ---\n",
    "    print(\"\\n--- Preparing Data ---\")\n",
    "    cifar_train_loader, cifar_test_loader = get_cifar100_loaders(\n",
    "        CONFIG[\"data_dir\"], CONFIG[\"img_size\"], CONFIG[\"batch_size\"]\n",
    "    )\n",
    "    tiny_imagenet_test_loader = get_tiny_imagenet_loaders(\n",
    "        CONFIG[\"data_dir\"], CONFIG[\"tiny_imagenet_dir\"], CONFIG[\"img_size\"], CONFIG[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # --- 2. Setup Model ---\n",
    "    print(\"\\n--- Initializing Model ---\")\n",
    "    if CONFIG[\"model_type\"] == \"kora\":\n",
    "        model = KoRAViT(\n",
    "            vit_model_name=CONFIG[\"vit_model\"],\n",
    "            rank=CONFIG[\"lora_rank\"],\n",
    "            num_classes=CONFIG[\"num_classes_cifar100\"],\n",
    "            composition_dim=CONFIG[\"composition_dim\"],\n",
    "            alpha=CONFIG[\"alpha\"]\n",
    "        ).to(CONFIG[\"device\"])\n",
    "    else: # LoRA\n",
    "        model = LoRAViT(\n",
    "            vit_model_name=CONFIG[\"vit_model\"],\n",
    "            rank=CONFIG[\"lora_rank\"],\n",
    "            num_classes=CONFIG[\"num_classes_cifar100\"],\n",
    "            alpha=CONFIG[\"alpha\"]\n",
    "        ).to(CONFIG[\"device\"])\n",
    "\n",
    "    # --- 3. Parameter Efficiency Analysis ---\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "    print(f\"Trainable Parameters ({CONFIG['model_type'].upper()}): {trainable_params / 1e6:.2f}M\")\n",
    "    print(f\"Efficiency Ratio: {trainable_params / total_params:.4%}\")\n",
    "    \n",
    "    # --- 4. Fine-tuning on CIFAR-100 ---\n",
    "    print(\"\\n--- Phase 1: Fine-tuning on CIFAR-100 ---\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=CONFIG[\"learning_rate\"])\n",
    "    \n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    \n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "        train_loss = train_one_epoch(model, cifar_train_loader, criterion, optimizer, CONFIG[\"device\"])\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, cifar_test_loader, criterion, CONFIG[\"device\"])\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']} -> \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    print(\"\\n--- CIFAR-100 Fine-tuning Results ---\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "    print(f\"Final Validation F1 Score: {val_f1:.4f}\")\n",
    "    # Stability (H1c Proxy): Lower variance in validation accuracy suggests a smoother landscape\n",
    "    val_acc_variance = np.var(history['val_acc'])\n",
    "    print(f\"Validation Accuracy Variance (Stability Proxy): {val_acc_variance:.6f}\")\n",
    "\n",
    "    # --- 5. CKA Analysis for Compositionality (H1a) ---\n",
    "    print(\"\\n--- Analyzing Compositionality with CKA (H1a) ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    # We will hook into the backbone blocks to get representations\n",
    "    cka_hooks = []\n",
    "    activations = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # output of a block is a tensor, we take the CLS token part\n",
    "            activations[name] = output[:, 0, :].detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    # Hook into first, middle, and last blocks\n",
    "    block_indices = [0, len(model.backbone.blocks) // 2, len(model.backbone.blocks) - 1]\n",
    "    for i in block_indices:\n",
    "        handle = model.backbone.blocks[i].register_forward_hook(get_activation(f'block_{i}'))\n",
    "        cka_hooks.append(handle)\n",
    "\n",
    "    # Collect activations from one batch\n",
    "    images, _ = next(iter(cifar_test_loader))\n",
    "    _ = model(images.to(CONFIG[\"device\"]))\n",
    "    \n",
    "    # Compute CKA matrix\n",
    "    block_names = list(activations.keys())\n",
    "    cka_matrix = np.zeros((len(block_names), len(block_names)))\n",
    "    for i in range(len(block_names)):\n",
    "        for j in range(len(block_names)):\n",
    "            cka_matrix[i, j] = cka(activations[block_names[i]], activations[block_names[j]])\n",
    "\n",
    "    print(\"CKA Similarity Matrix (between block outputs):\")\n",
    "    print(cka_matrix)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cka_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\", \n",
    "                xticklabels=[f\"Block {i}\" for i in block_indices], \n",
    "                yticklabels=[f\"Block {i}\" for i in block_indices])\n",
    "    plt.title(f\"CKA Representation Similarity ({CONFIG['model_type'].upper()})\")\n",
    "    plt.show()\n",
    "\n",
    "    for handle in cka_hooks: # Clean up CKA hooks\n",
    "        handle.remove()\n",
    "\n",
    "    # --- 6. Transferability to Tiny ImageNet (H1d) ---\n",
    "    print(\"\\n--- Phase 2: Zero-Shot Transfer to Tiny ImageNet (H1d) ---\")\n",
    "    # Note: The head was trained on CIFAR-100 classes, so this is a test of feature transfer,\n",
    "    # not direct classification. For a true test, we would retrain only the head.\n",
    "    # Here, we'll evaluate the feature quality via a proxy metric on the wrong head.\n",
    "    # This is a tough test and low scores are expected.\n",
    "    \n",
    "    transfer_loss, transfer_acc, transfer_f1 = evaluate(model, tiny_imagenet_test_loader, criterion, CONFIG[\"device\"])\n",
    "    print(\"\\n--- Tiny ImageNet Transfer Results (Zero-Shot) ---\")\n",
    "    print(f\"Performance on mismatched head -> Accuracy: {transfer_acc:.4f}, F1: {transfer_f1:.4f}\")\n",
    "    print(\"(Note: Low accuracy is expected as the classification head is for CIFAR-100)\")\n",
    "    \n",
    "    # A better test: replace the head and fine-tune for 1 epoch\n",
    "    print(\"\\n--- Fine-tuning Head on Tiny ImageNet (Few-Shot Proxy) ---\")\n",
    "    model.head = nn.Linear(model.head.in_features, CONFIG['num_classes_tiny_imagenet']).to(CONFIG['device'])\n",
    "    optimizer_transfer = optim.AdamW(model.head.parameters(), lr=1e-3)\n",
    "\n",
    "    # --- START OF THE FIX ---\n",
    "    \n",
    "    # 1. Define the transform for the training images\n",
    "    transform_tiny_train = transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # 2. Create the full Tiny ImageNet training dataset WITH the transform\n",
    "    tiny_train_dataset = timm.data.ImageDataset(\n",
    "        os.path.join(CONFIG[\"tiny_imagenet_dir\"], 'train'),\n",
    "        transform=transform_tiny_train\n",
    "    )\n",
    "    \n",
    "    # 3. Create the subset from the transformed dataset\n",
    "    tiny_train_subset = Subset(tiny_train_dataset, indices=list(range(1000))) # use 1000 samples for few-shot\n",
    "    \n",
    "    # --- END OF THE FIX ---\n",
    "\n",
    "    \n",
    "    # Create a small training subset of Tiny ImageNet\n",
    "    # tiny_train_subset = Subset(timm.data.ImageDataset(os.path.join(CONFIG[\"tiny_imagenet_dir\"], 'train')),\n",
    "                              # indices=list(range(1000))) # use 1000 samples for few-shot\n",
    "    tiny_train_loader_fs = DataLoader(tiny_train_subset, batch_size=CONFIG['batch_size'])\n",
    "    \n",
    "    train_one_epoch(model, tiny_train_loader_fs, criterion, optimizer_transfer, CONFIG['device'])\n",
    "    \n",
    "    fs_loss, fs_acc, fs_f1 = evaluate(model, tiny_imagenet_test_loader, criterion, CONFIG['device'])\n",
    "    print(\"\\n--- Tiny ImageNet Transfer Results (1-Epoch Head Fine-tuning) ---\")\n",
    "    print(f\"Few-Shot Transfer -> Accuracy: {fs_acc:.4f}, F1: {fs_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a76966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T17:57:58.098764Z",
     "iopub.status.busy": "2025-10-19T17:57:58.098299Z",
     "iopub.status.idle": "2025-10-19T18:49:09.983920Z",
     "shell.execute_reply": "2025-10-19T18:49:09.983046Z"
    },
    "papermill": {
     "duration": 3071.889794,
     "end_time": "2025-10-19T18:49:09.985081",
     "exception": false,
     "start_time": "2025-10-19T17:57:58.095287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:03<00:00, 46.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny ImageNet already downloaded.\n",
      "\n",
      "--- Initializing Model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afece386250f4bd2a9e1ebe2fad8bb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized KoRA with 48 adapters.\n",
      "Total Parameters: 88.16M\n",
      "Trainable Parameters (KORA): 1.59M\n",
      "Efficiency Ratio: 1.8038%\n",
      "\n",
      "--- Phase 1: Fine-tuning on CIFAR-100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 -> Train Loss: 1.3941, Val Loss: 0.7279, Val Accuracy: 0.7970, Val F1: 0.7970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 -> Train Loss: 0.6025, Val Loss: 0.6198, Val Accuracy: 0.8218, Val F1: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 -> Train Loss: 0.4893, Val Loss: 0.5781, Val Accuracy: 0.8331, Val F1: 0.8326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 -> Train Loss: 0.4198, Val Loss: 0.5689, Val Accuracy: 0.8338, Val F1: 0.8337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 -> Train Loss: 0.3680, Val Loss: 0.5461, Val Accuracy: 0.8408, Val F1: 0.8409\n",
      "\n",
      "--- CIFAR-100 Fine-tuning Results ---\n",
      "Final Validation Accuracy: 0.8408\n",
      "Final Validation F1 Score: 0.8409\n",
      "Validation Accuracy Variance (Stability Proxy): 0.000237\n",
      "\n",
      "--- Analyzing Compositionality with CKA (H1a) ---\n",
      "CKA Similarity Matrix (between block outputs):\n",
      "[[1.         0.5006395  0.39740276]\n",
      " [0.5006395  1.         0.75948519]\n",
      " [0.39740276 0.75948519 1.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Zero-Shot Transfer to Tiny ImageNet (H1d) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tiny ImageNet Transfer Results (Zero-Shot) ---\n",
      "Performance on mismatched head -> Accuracy: 0.0024, F1: 0.0048\n",
      "(Note: Low accuracy is expected as the classification head is for CIFAR-100)\n",
      "\n",
      "--- Fine-tuning Head on Tiny ImageNet (Few-Shot Proxy) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tiny ImageNet Transfer Results (1-Epoch Head Fine-tuning) ---\n",
      "Few-Shot Transfer -> Accuracy: 0.9834, F1: 0.9916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set model_type to 'lora' or 'kora' to run the desired experiment\n",
    "    # You can run this script twice, once for each setting, to compare results.\n",
    "    # CONFIG[\"model_type\"] = \"lora\" \n",
    "    # run_experiment()\n",
    "\n",
    "    CONFIG[\"model_type\"] = \"kora\"\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac7e5b",
   "metadata": {
    "papermill": {
     "duration": 0.784364,
     "end_time": "2025-10-19T18:49:11.470867",
     "exception": false,
     "start_time": "2025-10-19T18:49:10.686503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 56828,
     "sourceId": 109264,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3096.81685,
   "end_time": "2025-10-19T18:49:14.745916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-19T17:57:37.929066",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "209b553348244ae68300e56ed1437347": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "583a09c15ba84b308b1254b3406e9b49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_209b553348244ae68300e56ed1437347",
       "max": 346284714.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7efe7fd64a1c45a686d8536fe24f2a84",
       "tabbable": null,
       "tooltip": null,
       "value": 346284714.0
      }
     },
     "5d276a2c732347fd81e4793c3549c9b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e9cb1c50b58a4daa8b0571496253bff2",
       "placeholder": "​",
       "style": "IPY_MODEL_67094b9d3c014486b0e17c71144828a3",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "67094b9d3c014486b0e17c71144828a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "691651e34e7a44ec94c5e3b11774e15c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7efe7fd64a1c45a686d8536fe24f2a84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "81980474d37649618b628ee38a4bfbd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dead6a6b87c54486b95139e4d8ee0d9d",
       "placeholder": "​",
       "style": "IPY_MODEL_ea74739c87a442c79f3c8de24df4584d",
       "tabbable": null,
       "tooltip": null,
       "value": " 346M/346M [00:02&lt;00:00, 315MB/s]"
      }
     },
     "afece386250f4bd2a9e1ebe2fad8bb88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5d276a2c732347fd81e4793c3549c9b2",
        "IPY_MODEL_583a09c15ba84b308b1254b3406e9b49",
        "IPY_MODEL_81980474d37649618b628ee38a4bfbd8"
       ],
       "layout": "IPY_MODEL_691651e34e7a44ec94c5e3b11774e15c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "dead6a6b87c54486b95139e4d8ee0d9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9cb1c50b58a4daa8b0571496253bff2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea74739c87a442c79f3c8de24df4584d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
